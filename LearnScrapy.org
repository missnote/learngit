* 初窥scrapy
** 选择一个网站，http://www.mininova.org/today
** 定义要抓取的数据
#+BEGIN_SRC python

import scrapy
class TorrentItem(scrapy.Item):
    url = scrapy.Field()
    name = scrapy.Field()
    description = scrapy.Field()
    size = scrapy.Field()

#+END_SRC

** spider code
#+BEGIN_SRC python
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors import LinkExtractor

class MininovaSpider(CrawlSpider):

    name = 'mininova'
    allowed_domains = ['mininova.org']
    start_urls = ['http://www.mininova.org/today']
    rules = [Rule(LinkExtractor(allow=['/tor/\d+']), 'parse_torrent')]

    def parse_torrent(self, response):
        torrent = TorrentItem()
        torrent['url'] = response.url
        torrent['name'] = response.xpath("//h1/text()").extract()
        torrent['description'] = response.xpath("//div[@id='description']").extract()
        torrent['size'] = response.xpath("//div[@id='info-left']/p[2]/text()[2]").extract()
        return torrent
#+END_SRC
** 执行spider，获取数据
scrapy crawl mininova -o scrapy_data.json
** 查看提取的数据
emacs scrapy_data.json

* 安装指南
** 安装scrapy
pip install scrapy

* scrapy 入门教程
** 创建一个scrapy项目
scrapy startproject tutorial
#+BEGIN_SRC 项目tree
tutorial/
    scrapy.cfg
    tutorial/
        __init__.py
        items.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            ...
#+END_SRC
scrapy.cfg:项目配置文件
tutorial/项目Python模块，在此添加代码
tutorial/items.py:项目中的item文件
tutorial/pipelines.py:项目中的pipeline文件
tutorial/settings.py:项目中的配置文件
tutorial/spiders:放置spider代码
** 定义爬取的item
编辑tutorial目录中的items.py
#+BEGIN_SRC python
import scrapy

class DmozItem(scrapy.Item):
    title = scrapy.Field()
    link = scrapy.Field()
    desc = scrapy.Field()
#+END_SRC
** 编写spider并提取item
spider必须继承scrapy.Spider,定义以下三个属性
name:用于区别spider，name唯一。
start_urls:包含爬取的URL列表。
parse():spider的一个方法，负责解析返回的数据，提取数据，以及生成进一步处理的URL对象
新建文件，保存在tutorial/spiders目录下，dmoz_spider.py
#+BEGIN_SRC python
import scrapy

class DmozSpider(scrapy.spiders.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        filename = response.url.split("/")[-2]
        with open(filename, 'wb') as f:
            f.write(response.body)
#+END_SRC
*** 爬取
scrapy crawl dmoz
*** 提取item
scrapy使用xpath和css表达式机制
example：
/html/head/title:选择HTML文档中head标签内的title元素
/html/head/title/text():上面title元素的文字
//td:选择所有的td元素
//div[@class='mine']:选择所有的具有class=‘mine’的div元素
www.w3school.com.cn/xpath/index.asp
*** selectors选择器
xpath():传入xpath表达式，返回节点list
css():传入css表达式，返回节点list
extract():序列化改节点为Unicode字符，返回list
re():根据正则表达式对数据进行提取，返回Unicode list

*** 提取数据
在终端输入response.body查看HTML
#+BEGIN_SRC python
sel.xpath('//ul/li')
sel.xpath('//ul/li/text()').extract()
for sel in response.xpath('//ul/li'):
    title = sel.xpath('a/text()').extract()
    link = sel.xpath('a/@href').extract()
    desc = sel.xpath('text()').extract()
    print title, link, desc
#+END_SRC
#+BEGIN_SRC python
import scrapy

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            title = sel.xpath('a/text()').extract()
            link = sel.xpath('a/@href').extract()
            desc = sel.xpath('text()').extract()
            print title, link, desc
#+END_SRC
** 使用item
#+BEGIN_SRC python
import scrapy

from tutorial.items import DmozItem

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            item = DmozItem()
            item['title'] = sel.xpath('a/text()').extract()
            item['link'] = sel.xpath('a/@href').extract()
            item['desc'] = sel.xpath('text()').extract()
            yield item
#+END_SRC
** 编写item pipeline来保存获取到的数据
保存爬取到的数据
#+BEGIN_SRC python
scrapy crawl dmoz -o items.json
#+END_SRC
* 命令行工具
** 默认的scrapy的结构
#+BEGIN_SRC scrapy
scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
#+END_SRC
** 使用scrapy工具
#+BEGIN_SRC scrapy
Scrapy 1.3.0 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command
#+END_SRC
** 创建项目
scrapy startproject myproject
cd myproject
然后可以使用scrapy命令来控制项目

** 控制项目
创建一个新的spider：scrapy genspider mydomain mydomain.com
** 可用的scrapy命令
scrapy<command> -h
*** 全局命令：
startproject,settings,runspider,shell,fetch,view,version
*** 项目命令：
crawl,check,list,edit,parse,genspider,deploy,bench

*** genspider
在项目中创建spider
scrapy genspider [-t template] <name> <domain>
查看模板：scrapy genspider -l
查看详细信息：scrapy genspider -d basic
使用模板：scrapy genspider -t basic example example.com
*** crawl
语法：scrapy crawl <spider>
*** check
scrapy check [-l] <spider>
*** list
列出所有的spider：scrapy list
*** fetch
scrapy fetch url
scrapy fetch --nolog --headers https://www.baidu.com
*** view
在浏览器打开给定的URL
scrapy view url
*** settings
scrapy settings --get BOT_NAME
scrapy settings --get DOWNLOAD_DELAY
*** runspider
scrapy runspider <spider_file.py>
运行一个为创建项目的spider
*** version
scrapy version -v
Scrapy    : 1.3.0
lxml      : 3.7.1.0
libxml2   : 2.9.4
cssselect : 1.0.1
parsel    : 1.1.0
w3lib     : 1.16.0
Twisted   : 16.6.0
Python    : 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)]
pyOpenSSL : 16.2.0 (OpenSSL 1.0.2k  26 Jan 2017)
Platform  : Windows-7-6.1.7601-SP1
*** deploy
scrapy deploy [ <target:project> | -l <target> | -L ]
*** bench
scrapy bench
运行benchmark测试
* Items
Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法
** 声明item
#+BEGIN_SRC python items declear
import scrapy

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)
#+END_SRC
** 与Item配合
*** 创建item
#+BEGIN_SRC python
product = Product(name='Desktop PC', price=1000)
print product
Product(name='Desktop PC', price=1000)
#+END_SRC
*** 获取字段的值
#+BEGIN_SRC python
product['name']
Desktop PC
product.get('name')
Desktop PC

product['price']
1000

product['last_updated']
Traceback (most recent call last):
    ...
KeyError: 'last_updated'

product.get('last_updated', 'not set')
not set

product['lala'] # getting unknown field
Traceback (most recent call last):
    ...
KeyError: 'lala'

product.get('lala', 'unknown field')
'unknown field'

'name' in product  # is name field populated?
True

'last_updated' in product  # is last_updated populated?
False

'last_updated' in product.fields  # is last_updated a declared field?
True

'lala' in product.fields  # is lala a declared field?
False

#+END_SRC
*** 设置字段的值
#+BEGIN_SRC python
product['last_updated'] = 'today'
product['last_updated']
today

product['lala'] = 'test' # setting unknown field
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
#+END_SRC
*** 获取所有获取的值
product.keys()
product.items()
** 其他任务
*** 复制item
product2 = Product(product)
product3 = product2.copy()
*** 根据item创建字典
dict(product)
*** 根据字典创建item
Product({'name':'niuniu','price':'expensive'})
Product({'name':'niuniu','lala':'expensive'})#error
** 扩展item
继承原始的item来扩展item
#+BEGIN_SRC python
class DiscountedProduct(Product):
    discount_percent = scrapy.Field(serializer=str)
    discount_expiration_date = scrapy.Field()
#+END_SRC
* spiders
定义爬取动作，分析网页的地方
spider中初始的request是通过调用 start_requests() 来获取的。
start_requests() 读取 start_urls 中的URL， 并以 parse 为回调函数生成 Request 。
** spider参数
在运行crawl时，添加-a可以传递spider参数：
scrapy crawl myspider -a category=electronics
spider在构造中获取参数
#+BEGIN_SRC python
import scrapy

class MySpider(Spider):
    name = 'myspider'

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = ['http://www.example.com/categories/%s' % category]
        # ...
#+END_SRC
** 内置spider参考手册
*** spider
Spider并没有提供什么特殊的功能。 其仅仅请求给定的 start_urls/start_requests，
并根据返回的结果(resulting responses)调用spider的 parse 方法。
name:定义spider名字的字符串，唯一。
allowed_domains:允许爬取的域名列表
start_urls:url列表，没有特定的URL时，从该列表开始爬取。
*** custom_settings
for detail: http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/settings.html#topics-settings-ref
*** crawler
for detail: http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/api.html#topics-api-crawler
*** start_requests()
返回一个可迭代的对象，默认使用start_urls的url生成requests.
可以重写该方法
#+BEGIN_SRC python
def start_requests(self):
    return [scrapy.FormRequest("http://www.example.com/login",
                               formdata={'user': 'john', 'pass': 'secret'},
                               callback=self.logged_in)]

def logged_in(self, response):
    # here you would extract links to follow and return Requests for
    # each of them, with another callback
    pass
#+END_SRC
*** parse(response)
当没有指定callback函数时，这个方法是默认的回调函数
*** log()
for detail:http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/logging.html#topics-logging
*** closed()
spider关闭时调用
** spider样例
#+BEGIN_SRC python
import scrapy

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        self.log('A response from %s just arrived!' % response.url)
#+END_SRC

#+BEGIN_SRC python
import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        sel = scrapy.Selector(response)
        for h3 in response.xpath('//h3').extract():
            yield MyItem(title=h3)

        for url in response.xpath('//a/@href').extract():
            yield scrapy.Request(url, callback=self.parse)
#+END_SRC
** crawlspider样例
#+BEGIN_SRC python
import scrapy
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = (
        # 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True)
        Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

        # 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析
        Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

    def parse_item(self, response):
        self.log('Hi, this is an item page! %s' % response.url)

        item = scrapy.Item()
        item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)')
        item['name'] = response.xpath('//td[@id="item_name"]/text()').extract()
        item['description'] = response.xpath('//td[@id="item_description"]/text()').extract()
        return item
#+END_SRC
** XMLFeedSpider
#+BEGIN_SRC python
from scrapy import log
from scrapy.contrib.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.xml']
    iterator = 'iternodes' # This is actually unnecessary, since it's the default value
    itertag = 'item'

    def parse_node(self, response, node):
        log.msg('Hi, this is a <%s> node!: %s' % (self.itertag, ''.join(node.extract())))

        item = TestItem()
        item['id'] = node.xpath('@id').extract()
        item['name'] = node.xpath('name').extract()
        item['description'] = node.xpath('description').extract()
        return item
#+END_SRC
** CSVFeedSpider
#+BEGIN_SRC python
from scrapy import log
from scrapy.contrib.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.csv']
    delimiter = ';'
    quotechar = "'"
    headers = ['id', 'name', 'description']

    def parse_row(self, response, row):
        log.msg('Hi, this is a row!: %r' % row)

        item = TestItem()
        item['id'] = row['id']
        item['name'] = row['name']
        item['description'] = row['description']
        return item
#+END_SRC
* 选择器
BeautifulSoup,lxml,xpath
** 使用选择器
构造选择器
#+BEGIN_SRC python
>>> from scrapy.selector import Selector
>>> from scrapy.http import HtmlResponse
#+END_SRC
用文字构造
#+BEGIN_SRC python
>>> body = '<html><body><span>good</span></body></html>'
>>> Selector(text=body).xpath('//span/text()').extract()
#+END_SRC
用response构造
#+BEGIN_SRC python
>>> response = HtmlResponse(url='http://example.com', body=body)
>>> Selector(response=response).xpath('//span/text()').extract()
#+END_SRC
用select属性
#+BEGIN_SRC python
>>> response.selector.xpath('//span/text()').extract()
#+END_SRC
** 嵌套选择器
** 正则表达式
#+BEGIN_SRC python
response.xpath('//a[contains(@href, "image")]/text()').re(r'Name:\s*(.*)')
#+END_SRC
** 使用相对路径
绝对路径以/开始
#+BEGIN_SRC python
>>> divs = response.xpath('//div')
>>> for p in divs.xpath('//p'):  # this is wrong - gets all <p> from the whole document
...     print p.extract()
>>> for p in divs.xpath('.//p'):  # extracts all <p> inside
...     print p.extract()
>>> for p in divs.xpath('p'):
...     print p.extract()
#+END_SRC
** 使用建议
*** Using text nodes in a condition
When you need to use the text content as argument to a XPath string function,
avoid using .//text() and use just . instead.
*** Beware the difference between //node[1] and (//node)[1]
//node[1] selects all the nodes occurring first under their respective parents
(//node)[1] selects all the nodes in the document, and then gets only the first of them.
*** When querying by class, consider using CSS
#+BEGIN_SRC python
>>> from scrapy import Selector
>>> sel = Selector(text='<div class="hero shout"><time datetime="2014-07-23 19:00">Special date</time></div>')
>>> sel.css('.shout').xpath('./time/@datetime').extract()
[u'2014-07-23 19:00']
#+END_SRC
* Item Loaders
Item Loaders提供的是一种灵活，高效的机制，可以更方便的被spider或source format (HTML, XML, etc)扩展，
并override更易于维护的、不同的内容分析规则.
** Using Item Loaders to populate items
#+BEGIN: clocktable :maxlevel 2 :scope subtree
#+CAPTION: Clock summary at [2017-05-27 周六 11:16]
| Headline     | Time   |   |
|--------------+--------+---|
| *Total time* | *0:00* |   |
#+END:
